# Convolutional-Neural-Network

卷积神经网络（Convolutional Neural Network，CNN）的提出最初是为解决图像识别问题，但是随着技术的发展，现在CNN的应用也不仅局限于图像和视频，也可以用于音频和文本等时间序列数据。CNN作为一个深度学习架构被提出的最初目的是为了降低对图像数据预处理的要求，避免复杂的特征工程。CNN的最大特点在于卷积的权值共享结构，可以减少神经网络的参数量，防止过拟合的同时也减少了神经网络模型的复杂度。一个卷积层中可以有多个卷积核，而每个卷积核都对应一个滤波后映射出新的图像，同一个新图像中每个像素都来自完全相同的卷积核，这就是权值共享。每个卷积核可以看成一个特征提取器，使用不同的卷积核是为了提取到更多的不同特征。CNN的前提假设是元素之间是相互独立的、输入和输出是相互独立的。

## CNN的结构

### 1、输入层

卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组  。由于卷积神经网络在计算机视觉领域有广泛应用，因此许多研究在介绍其结构时预先假设了三维输入数据，即平面上的二维像素点和RGB通道。

### 2、隐含层

卷积神经网络的隐含层包含卷积层、池化层和全连接层3类常见构筑，在一些更为现代的算法中可能有Inception模块、残差块（residual block）等复杂构筑。在常见构筑中，卷积层和池化层为卷积神经网络特有。卷积层中的卷积核包含权重系数，而池化层不包含权重系数，因此在文献中，池化层可能不被认为是独立的层。以LeNet-5为例，3类常见构筑在隐含层中的顺序通常为：输入-卷积层-池化层-卷积层-池化层-全连接层-输出。

2.1、卷积层

2.1.1、卷积核

一个卷积层中可以有多个不同的卷积核，而每个卷积核都对应一个滤波后映射出新的图像。

2.1.2、卷积层参数

卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数 。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂 。卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充（replication padding）。填充依据其层数和目的可分为两类：

- 有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积（narrow convolution）”，窄卷积输出的特征图尺寸为(L-f)/s+1。

- 相同填充/半填充（same padding）：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积（equal-width convolution）”。

2.1.3、激活函数

激活函数f是非线性的，它的作用是将非线性引入到神经元的输出中，以此达到神经元学习非线性表达的目的。常见的激活函数有relu、sigmoid函数、tanh函数。

![1558420821](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420821.jpg)

左边的是原始图，大小为5x5

中间的是卷积核，大小为3x3

右边的是特征图，大小为3x3

原始图像nxn，用fxf的卷积核进行卷积，填充层数为p=0，步长s=1，卷积后得到的特征图大小为[(n+2xp-f)/s+1]x[(n+2xp-f)/s+1]（向下取整）。

上述卷积操作后，图像大小会减小，边缘信息可能会丢失，为了解决这些问题，就引入padding填充操作，在卷积之前，沿着边缘填充0，填充层数p=(f-1)/2，padding后可以保证卷积后得到的特征图和原始图像的大小相同。

![1558420861(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420861(1).jpg)

左上的是原始图，大小为4x4，padding后，大小为6x6

右上的是卷积核，大小为3x3

右下的是特征图，大小为4x4

原始图像nxn，用fxf的卷积核进行卷积，填充层数为p=1，步长s=1，卷积后得到的特征图大小为[(n+2xp-f)/s+1]x[(n+2xp-f)/s+1]（向下取整）。

2.2、池化层

2.2.1、最大池化层

maxpooling：取滑动窗口里最大的值

![1558420886(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420886(1).jpg)

2.2.2、平均池化层

averagepooling：取滑动窗口内所有值的平均值

![1558420918(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420918(1).jpg)

2.3、flatten层和全连接层

多个卷积核对图像进行卷积后，然后进行池化，得到多个特征图，经过flatten层，把特征图“拉平”，再到下一层全连接层。卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去3维结构，被展开为向量并通过激励函数传递至下一层。

![1558420955(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420955(1).jpg)

## 3、输出层

卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数（softmax function）输出分类标签。在物体识别（object detection）问题中，输出层可设计为输出物体的中心坐标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。

![1558420979(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420979(1).jpg)

总结一些问题

（1）传统的神经网络存在什么问题？

权值参数太多，计算量大，模型越复杂，调参越困难，模型越容易过拟合。神经网络在反向传播过程中，梯度越来越小，可能会出现梯度消失问题，梯度一旦趋于0，权值无法更新，这个神经元就不起作用。

（2）为什么使用卷积神经网络？它有什么优缺点？

优点：权值共享，可以减少神经网络的参数数量，防止过拟合的同时，也降低了模型的复杂度。不需要进行复杂的特征工程，卷积神经网络可以自动进行特征提取。

缺点：需要调参，需要大量的训练样本（可能需要用到GPU），可解释性差（神经网络的共同“特点”）。

（3）卷积核的尺寸必须是正方形吗？

卷积核的尺寸不一定是正方形的。长方形也可以，不过在通常情况下设置为正方形。如果设置为长方形，那么首先得保证这层的输出形状是整数，不能是小数。

（4）卷积核的个数如何确定？每一层的卷积核的个数都是相同的吗？

卷积核的个数一般根据经验来设置，一般离输入层越近的卷积层的卷积核个数稍微少些，比如32个，离输出层越近的卷积层的卷积核个数稍微多一些，比如64个。

（5）步长的向右和向下移动的幅度必须是一样的吗？

不一定，参数都可以自己设置。

（6）padding操作有什么作用？

padding操作可以使得卷积后得到的特征图和卷积之前的图像大小相同，可以保留更多的边缘信息。

（7）为什么使用池化层？

池化层主要用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。

最后，用CNN来实现手写数字的识别，准确率为99.2%。数据来源：https://www.kaggle.com/c/digit-recognizer/data

网络结构：

Conv2d
