# Convolutional-Neural-Network

卷积神经网络（Convolutional Neural Network，CNN）的提出最初是为解决图像识别问题，但是随着技术的发展，现在CNN的应用也不仅局限于图像和视频，也可以用于音频和文本等时间序列数据。CNN作为一个深度学习架构被提出的最初目的是为了降低对图像数据预处理的要求，避免复杂的特征工程。CNN的最大特点在于卷积的权值共享结构，可以减少神经网络的参数量，防止过拟合的同时也减少了神经网络模型的复杂度。

## CNN的结构

### 1、输入层

卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组  。由于卷积神经网络在计算机视觉领域有广泛应用，因此许多研究在介绍其结构时预先假设了三维输入数据，即平面上的二维像素点和RGB通道。

### 2、隐含层

卷积神经网络的隐含层包含卷积层、池化层和全连接层3类常见构筑，在一些更为现代的算法中可能有Inception模块、残差块（residual block）等复杂构筑。在常见构筑中，卷积层和池化层为卷积神经网络特有。卷积层中的卷积核包含权重系数，而池化层不包含权重系数，因此在文献中，池化层可能不被认为是独立的层。以LeNet-5为例，3类常见构筑在隐含层中的顺序通常为：输入-卷积层-池化层-卷积层-池化层-全连接层-输出。

2.1、卷积层

2.1.1、卷积核

一个卷积层中可以有多个不同的卷积核，而每个卷积核都对应一个滤波后映射出新的图像。

2.1.2、卷积层参数

卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数 。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂 。卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。由卷积核的交叉相关计算可知，随着卷积层的堆叠，特征图的尺寸会逐步减小，例如16×16的输入图像在经过单位步长、无填充的5×5的卷积核后，会输出12×12的特征图。为此，填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充（replication padding）。填充依据其层数和目的可分为两类：

- 有效填充（valid padding）：即完全不使用填充，卷积核只允许访问特征图中包含完整感受野的位置。输出的所有像素都是输入中相同数量像素的函数。使用有效填充的卷积被称为“窄卷积（narrow convolution）”，窄卷积输出的特征图尺寸为(L-f)/s+1。

- 相同填充/半填充（same padding）：只进行足够的填充来保持输出和输入的特征图尺寸相同。相同填充下特征图的尺寸不会缩减但输入像素中靠近边界的部分相比于中间部分对于特征图的影响更小，即存在边界像素的欠表达。使用相同填充的卷积被称为“等长卷积（equal-width convolution）”。

2.1.3、激活函数

激活函数f是非线性的，它的作用是将非线性引入到神经元的输出中，以此达到神经元学习非线性表达的目的。常见的激活函数有relu、sigmoid函数、tanh函数。

![1558420821](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420821.jpg)

左边的是原始图，大小为5x5

中间的是卷积核，大小为3x3

右边的是特征图，大小为3x3

原始图像nxn，用fxf的卷积核进行卷积，填充层数为p=0，步长s=1，卷积后得到的特征图大小为[(n+2xp-f)/s+1]x[(n+2xp-f)/s+1]（向下取整）。

上述卷积操作后，图像大小会减小，边缘信息可能会丢失，为了解决这些问题，就引入padding填充操作，在卷积之前，沿着边缘填充0，填充层数p=(f-1)/2，padding后可以保证卷积后得到的特征图和原始图像的大小相同。

![1558420861(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420861(1).jpg)

左上的是原始图，大小为4x4，padding后，大小为6x6

右上的是卷积核，大小为3x3

右下的是特征图，大小为4x4

原始图像nxn，用fxf的卷积核进行卷积，填充层数为p=1，步长s=1，卷积后得到的特征图大小为[(n+2xp-f)/s+1]x[(n+2xp-f)/s+1]（向下取整）。

2.2、池化层

2.2.1、最大池化层

![1558420886(1)](https://github.com/LeungFe/Convolutional-Neural-Network/blob/master/images/1558420886(1).jpg)

2.2.2、平均池化层

2.3、全连接层

积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层通常搭建在卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去3维结构，被展开为向量并通过激励函数传递至下一层。

## 3、输出层

卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数（softmax function）输出分类标签。在物体识别（object detection）问题中，输出层可设计为输出物体的中心坐标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。
